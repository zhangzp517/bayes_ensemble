---
output:
  pdf_document:
    fig_height: 7
    fig_width: 7
author: 'Zachary Gallegos and Saman Sirafi'
title: 'A Bayesian Approach for Online Classifier Ensemble (Bai, Lam, and Sclaroff)'
abstract: 'This paper reviews a technique presented in "A Bayesian Approach for Online Classifier Ensemble" (Bai, Lam, and Sclaroff) for optimizing the selection of ensemble weights in an online classification setting. The presentation is Bayesian in fashion and involves defining a likelihood distribution for the losses of each classifier and a prior distribution for the ensemble weights, and updating the weights at each step based on their posterior distribution. The authors experiments (and the reproductions in this paper) show that this method often compares favorably to a single online classifier and other ensemble weighting schemes, such as uniform voting and stochastic gradient descent (SGD).'
---

# Introduction

In classification problems, ensembling is a popular technique in machine learning that involves aggregating many "weak" classifiers into a "strong" one that, ideally, outperforms any of the individual classifiers on its own. In the interest of optimizing the aggregation process, individual classifiers in the ensemble are often weighted in some fashion that is intended to produce more accurate predictions. In a standard offline setting (that is, one in which the model is trained once and new data is not added to the trained model in any fashion) many methods for choosing the ensemble weights have been investigated.

# A Bayesian Method for Optimizing Online Ensemble Weights

Now we will review the Bayesian method proposed for optimizing the ensemble weights, the subject of this paper. First, define the following:

\begin{align}
& \left\{ \left(\mathbf{x}^{t}, y^t \right) \right\} && \text{ training samples at time } t \nonumber \\
& \left\{ c^{t}_{i}\left(\mathbf{x}\right) \right\}_{i=1}^{m} && \text{ set of } m \text{ classifiers in ensemble at } t \nonumber \\
& \mathbf{\lambda} && \text{ vector of ensemble weights, } \lambda_i > 0 \nonumber \\ \\
& \mathbf{g} = \left(g_1, \ldots, g_m \right) && \text{ vector of losses for each classifier, } g_i \geq 0 \nonumber \\
& \mathbf{g}^t = \left(g^{t}_1, \ldots, g^{t}_m \right) && \text{ vector of losses at } t \nonumber \\
& \mathbf{g}^{1:t} = \left(\mathbf{g}^{1:t}_1, \ldots, \mathbf{g}^{1:t}_m \right) && \text{ matrix of losses up to } t \nonumber \\
& l \left(\mathbf{\lambda}, \mathbf{g}^{t} \right) && \text{ loss function of the ensemble} \nonumber \\ \\
& p\left(\mathbf{g} \big| \mathbf{\lambda} \right) = e^{-l\left(\mathbf{\lambda}, \mathbf{g}\right)} && \text{likelihood function for the vector of losses} \nonumber \\
& p\left(\mathbf{\lambda}\right) = e^{-l_{0}\left(\mathbf{\lambda}\right)} && \text{prior for the ensemble weight vector} \nonumber
\end{align}

Our objective is to choose the ensemble weight vector $\mathbf{\lambda}$ to minimize the expected value of the ensemble loss function, $E\left[l \left(\mathbf{\lambda}, \mathbf{g}^{t} \right)\right]$, which we estimate by $\sum_{i=1}^{t} l \left(\mathbf{\lambda}, \mathbf{g}^{i} \right)$.





# Empirical

This section reproduces experiments performed in the Section 5 of the paper ("Experiments") on test datasets to compare the Bayesian method proposed to other ensembling techniques. Our results match those of the authors closely. Data utilized were from five benchmark classification datasets ("heart", "mushrooms", "australian", "ionosphere", and "sonar") available here: https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html. All datasets present a binary classification problem. Full (Python) code for the experiment can be found in the appendix. (Note that the datasets as downloaded from the website are in sparse matrix format, which are most easily read using the scikit-learn function sklearn.datasets.load_svmlight_file.)

Each experiment runs as follows: 

For each trial, the test dataset in question is randomly split into an initial training portion (we use 10% of the available data, as is used in the paper) and a testing portion (the remaining 90% of the data) to be processed a single sample at a time in an online fashion. A single baseline naive bayes classifier that uses all the features is created, and a single ensemble of one-hundred naive bayes classifiers is created. To ensure differentiation in the classifiers in the ensemble, each is created to use a random subset of half of the available features in the data. The authors do not state in their paper how many features are used for their ensemble classifiers, and the number of features between datasets differs widely. This may be a source of small differences in results, though it does not seem to make a significant difference. This ensemble is then shared among the three ensembling methods considered:

1. "Voting" uses soft voting (class chosen by $argmax$ of the sums of the probability estimates for each class from each classifier in the ensemble).
2. "SGD" estimates the ensemble weights by stochastic gradient descent.
3. "Bayes" estimates the ensemble weights by the method described in this paper and make the prediction by the $argmin$ of the sums of the weighted losses for each class. Hyperparameters are set at $\alpha = 1$, $\beta = 1$, and $\theta = .1$. These are the values used in the final analysis given in the paper, and the authors show that the results of the experiments are practically invariant to the values of the hyperparameters, so we omit tuning of them.

In brief, the outline of the experiment is given below:

* Initialize: one baseline classifier, one ensemble
* For trials $i = 1, \ldots, T$:
    1. Split the dataset into initial training (10%) and incoming testing (90%). Suppose the testing data has $N$ samples
    2. Train all classifiers on the original data. Classifiers in the ensemble are trained on their particular subset of the features
    3. For samples $(\mathbf{X}_j, y_j)$, $j = 1, \ldots, N$:
        * Make a prediction for $y_j$ using the features $\mathbf{X}_j$ for each method
        * Reveal $y_j$ and partially fit each classifier on the new sample. Record cumulative error rate up to time $j$
    4. Calculate the overall error rate on the incoming samples and store cumulative error rates as time progresses

Three main conclusions are evident:

* Ensembling methods do not always appear warranted, and often do not improve significantly upon the single online classifier that uses all the features. This is especially true in datasets where most of the features seem to be relevant (e.g., the "heart" dataset), as the performance of the ensembling classifiers that use only a subset of them will suffer significantly, often more than the reduced variance from ensembling can overcome. The uniform voting scheme performance deteriorates rapidly in this setting, since it has no way of weighting the classifiers in the ensemble that are performing well relative to the others.
* The Bayesian method almost uniformly outperforms the uniform voting ensemble on each trial and each dataset. This is consistent with results presented in the paper that show the Bayesian method, under certain regularity conditions, is guaranteed to converge to a global minimum in the ensemble loss function, an attractive quality that neither voting nor SGD possesses.
* The Bayesian method seems to perform particularly strongly in high-dimensional datasets, functioning as a sort of feature-selection process for weighting the classifiers in the ensemble that contain the important features. This is evident in the "mushrooms" dataset, which contains 112 features, and on which the Bayesian ensemble consistently improves upon the single baseline classifier by about a percentage point, but the uniform voting ensemble consistently underperforms it by about the same margin. This appears to be because the subsetting of features harms the ensemble significantly when it cannot reweight itself towards the more important features.


```{r}
#'

suppressPackageStartupMessages(library(data.table))
suppressPackageStartupMessages(library(ggplot2))
suppressPackageStartupMessages(library(knitr))
suppressPackageStartupMessages(library(scales))

acc_path <- '/Users/zgallegos/documents/school/math_538/project/data/results/accuracy/'
cum_path <- '/Users/zgallegos/documents/school/math_538/project/data/results/cumulative/'

read <- function(x, id)
{
    d <- read.csv(x, header = TRUE)
    d$X <- NULL
    melt(d, id = id, value.name = 'error', variable.name = 'Method')
}

err_plot <- function(dataset)
{

    fl <- paste0(acc_path, dataset, '_errors.csv')

    data <- read(fl, 'trial')

    proper <- gsub('(?<=\\b)([a-z])', '\\U\\1', tolower(dataset), perl = TRUE)
    ttl <- paste0(proper, ' - Error Rates By Method/Trial')

    y_lower <- .975 * min(data$error)
    y_max <- 1.025 * max(data$error)

    data$trial <- data$trial + 1

    g <- ggplot(data = data,
            aes(x = as.factor(trial), y = error, fill = Method, group = Method))
    g <- g + geom_bar(stat = 'identity', position = 'dodge')
    # g <- g + geom_text(aes(label = percent(error)), 
    #                     position = position_dodge(width = 1),
    #                     family = 'serif',
    #                     vjust = 0,
    #                     size = 3)
    g <- g + coord_cartesian(ylim = c(y_lower, y_max))
    g <- g + theme_bw() + theme(text = element_text(family = 'serif'))
    g <- g + ggtitle(ttl) + xlab('Trial') + ylab('Error')
    g

}

cum_plot <- function(dataset, trial)
{

    fl <- paste0(cum_path, dataset, '_cumerrors_trial_', trial, '.csv')
    data <- read(fl, 'index')

    proper <- gsub('(?<=\\b)([a-z])', '\\U\\1', tolower(dataset), perl = TRUE)
    ttl <- paste0(proper, ' - Cumulative Error, Trial ', trial)

    g <- ggplot(data = data, aes(x = index, y = error, colour = Method))
    g <- g + geom_line()
    g <- g + theme_bw() + theme(text = element_text(family = 'serif'))
    g <- g + ggtitle(ttl) + xlab('Sample') + ylab('Cumulative Error')
    g

}

mns <- function(dataset)
{

    fl <- paste0(acc_path, dataset, '_errors.csv')
    data <- as.data.table(read(fl, 'trial'))

    kable(data[, .(`Mean Error` = mean(error)), by = .(Method)])

}


```

## Dataset - Australian

```{r}

err_plot('australian')

```

```{r}

mns('australian')

```

```{r}

cum_plot(dataset = 'australian', trial = 6)

```

## Dataset - Mushrooms

```{r}

err_plot('mushrooms')

```

```{r}

mns('mushrooms')

```

```{r}

cum_plot(dataset = 'mushrooms', trial = 4)

```

## Dataset - Ionosphere

```{r}

err_plot('ionosphere')

```

```{r}

mns('ionosphere')

```

```{r}

cum_plot(dataset = 'ionosphere', trial = 4)

```

## Dataset - Heart

```{r}

err_plot('heart')

```

```{r}

mns('heart')

```

```{r}

cum_plot(dataset = 'heart', trial = 2)

```

## Dataset - Sonar

```{r}

err_plot('sonar')

```

```{r}

mns('sonar')

```

```{r}

cum_plot(dataset = 'sonar', trial = 4)

```

# Appendix

## Empirical Section Full Code

The results shown in the Empirical section were produced by the following Python code:

```{python, eval = FALSE}

"""
not run
"""

import numpy as np
import pandas as pd
import os
import re

from sklearn import naive_bayes
from sklearn.base import clone
from sklearn.model_selection import train_test_split
from sklearn.metrics import log_loss, accuracy_score
from sklearn.datasets import load_svmlight_file


def create_ensemble(classifier, X, num, num_features):
    """
    create a uniform ensemble of <num> classifiers, each of which uses
    <num_features> randomly-sampled features of the data
    """
    estimators = [None] * num
    n_feats = X.shape[1]

    for i in range(num):

        estimators[i] = clone(classifier)
        estimators[i].sub_feats = list(np.sort(np.random.choice(
            range(n_feats), size = num_features, replace = False)))

    return estimators


def fit_ensemble(ensemble, x, y, partial = False, classes = None):
    """
    fit the ensemble, each classifier with its own set of features
    if <partial>, do an online fit
    """
    for e in ensemble:

        x_ = x[:, e.sub_feats]
        
        if not partial:

            e.fit(x_, y)

        else:

            e.partial_fit(x_, y, classes = classes)

    return ensemble


def update_bayes(ensemble, i, x, y, classes, a, b, theta):
    """
    update loss and ensemble weights
    """
    t = i + 1

    for e in ensemble:

        x_ = x[:, e.sub_feats]
        pred = e.predict_proba(x_)
        e.loss[i] = log_loss(y, pred, labels = classes)

        num = a + t
        den = b + theta * np.sum(e.loss[0:t])
        e.lam[i] = num / den

    return ensemble


def predict_bayes(ensemble, i, X, classes):
    """
    ensemble prediction using posterior ensemble weights
    """
    m = len(ensemble)
    neg_label, pos_label = classes
    
    if i == -1:

        return predict_vote(ensemble, X, classes)

    curr_lams = np.array([e.lam[i] for e in ensemble])

    loss_neg = [None] * m
    loss_pos = [None] * m

    for i, e in enumerate(ensemble):

        pred = e.predict_proba(X[:, e.sub_feats])

        loss_neg[i] = log_loss([neg_label], pred, labels = classes)
        loss_pos[i] = log_loss([pos_label], pred, labels = classes)

    loss_neg = np.array(loss_neg)
    loss_pos = np.array(loss_pos)

    probs = [sum(curr_lams * loss_neg), sum(curr_lams * loss_pos)]

    return classes[np.argmin(probs)]


def predict_vote(ensemble, X, classes):
    """
    ensemble prediction by soft voting
    """
    ensemble_preds = [e.predict_proba(X[:, e.sub_feats]) for e in ensemble]
    pred = np.concatenate(ensemble_preds).sum(axis = 0)

    return classes[np.argmax(pred)]


def online_test(ensemble, single, X, y, classes, start_prop, **kwargs):
    """
    perform the test
    """
    X_start, X_stream, y_start, y_stream = train_test_split(
        X, y, test_size = 1 - start_prop)

    stream_size = X_stream.shape[0]

    single_preds = [None] * stream_size
    vote_preds = [None] * stream_size
    bayes_preds = [None] * stream_size

    for e in ensemble:

        e.loss = [None] * stream_size
        e.lam = [None] * stream_size


    """
    initial fit
    """

    single.fit(X_start, y_start)
    ensemble = fit_ensemble(ensemble, X_start, y_start)

    for j in range(stream_size):

        x_ = X_stream[j].reshape(1, -1)
        y_ = y_stream[j].reshape(1,)

        """
        update bayes loss and weights
        """

        ensemble = update_bayes(ensemble, j, x_, y_, classes, **kwargs)

        """
        make predictions
        """

        single_preds[j] = single.predict(x_)
        vote_preds[j] = predict_vote(ensemble, x_, classes)
        bayes_preds[j] = predict_bayes(ensemble, j - 1, x_, classes)

        """
        partial fit
        """

        single.partial_fit(x_, y_, classes = classes)
        ensemble = fit_ensemble(ensemble, x_, y_, partial = True, classes = classes)

    single_preds = np.array(single_preds).reshape(y_stream.shape)
    vote_preds = np.array(vote_preds).reshape(y_stream.shape)
    bayes_preds = np.array(bayes_preds).reshape(y_stream.shape)

    return single_preds, vote_preds, bayes_preds, y_stream



if __name__ == '__main__':

    """
    run the tests. five data files, ten trials each
    """

    os.chdir('/users/zgallegos/documents/school/math_538/project/data')

    use_datasets = ['heart.txt', 'mushrooms.txt', 'australian.txt',
                    'ionosphere.txt', 'sonar.txt']
    use_classes = [(-1, 1), (1, 2), (-1, 1), (-1, 1), (-1, 1)]

    n_trials = 10

    """
    percentage of the features ensemble classifiers get
    we use half of them, but the authors do not state how many they use
    results may differ slightly based on this
    """
    feat_perc = .5

    for j, k in enumerate(use_datasets):
        
        fl_name = re.search('^.+?(?=\.txt)', k).group(0)
        err_fl = fl_name + '_errors.csv'

        cls = use_classes[j]

        X, y = load_svmlight_file(k)
        X = X.toarray()
        y = y.astype(int)

        feats = int(np.floor(feat_perc * X.shape[1]))

        sing = []
        vote = []
        bayes = []

        trlz = range(n_trials)

        for i in trlz:

            cumerr_fl = fl_name + '_cumerrors_trial_%s.csv' % i

            single = naive_bayes.BernoulliNB()
            ensemble = create_ensemble(naive_bayes.BernoulliNB(), X, 100, feats)

            s, v, b, y_ = online_test(ensemble, single, X, y, cls, 
                            start_prop = .1, a = 1, b = 1, theta = .1)

            indx = range(1, len(y_) + 1)
            cum_sing = 1 - np.cumsum(s == y_) / indx
            cum_vote = 1 - np.cumsum(v == y_) / indx
            cum_bayes = 1- np.cumsum(b == y_) / indx

            to_write = np.stack((indx, cum_sing, cum_vote, cum_bayes), axis = 1)
            
            df = pd.DataFrame(to_write, columns = ['index', 'single', 'vote', 'bayes'])
            df.to_csv(os.path.join('results', 'cumulative', cumerr_fl))

            sing.append(1 - accuracy_score(y_, s))
            vote.append(1 - accuracy_score(y_, v))
            bayes.append(1 - accuracy_score(y_, b))

        to_write = np.stack((trlz, sing, vote, bayes), axis = 1)

        df = pd.DataFrame(to_write, columns = ['trial', 'single', 'vote', 'bayes'])
        df.to_csv(os.path.join('results', 'accuracy', err_fl))

```