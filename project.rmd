---
output:
  pdf_document:
    fig_height: 6
    fig_width: 10
author: 'Zachary Gallegos and Saman Sirafi'
title: 'An Overview of "A Bayesian Approach for Online Classifier Ensemble" by (Bai, Lam, and Sclaroff)'
abstract: 'This paper reviews a technique presented in "A Bayesian Approach for Online Classifier Ensemble" (Bai, Lam, and Sclaroff) for optimizing the selection of ensemble weights in an online classification setting. The presentation is Bayesian in fashion and involves defining a likelihood distribution for the losses of each classifier and a prior distribution for the ensemble weights, and updating the weights at each step based on their posterior distribution. The authors experiments (and the reproductions in this paper) show that this method often compares favorably to a single online classifier and other ensemble weighting schemes, such as uniform voting and stochastic gradient descent (SGD).'
---

# Introduction


In classification problems, ensembling is a popular technique in statital and machine learning approaches. Some commonly known algorithms are ensemble methods such as Random Forest. Random Forest is an ensemble of decision trees that then yield the mode of the tree results for classification problems or the mean in the case of Regression problems. Although ensembling methods are used in many different ways, some as simples as means or modes; other methods use a second level of supervised learning algorithm to combine the results. In ensembling the first layer of algorithms are called 'weak' in reference to their lack of ability to make predictions effectively and the ensemble produces a 'strong' method, may it be a classification or regression problem. Without loss of generality we will focus on classification problems which is the specific ensemble method presented in this paper.

One challenge when ensembling with a new algorithm is the prospect of overfitting. To avoid this problem each weak classifier must differ from the others, thus a common practice is bagging(bootstrap aggregate). In bagging random subsets of the predictors are selected to build each of the weak classifier. This method is used in creating the decision trees of a Random Forest as well. 

This paper will be exploring the optimization of the ensemble process for classification using a bayesian ensemble method. 

In a standard offline setting (that is, one in which the model is trained once and new data is not added to the trained model in any fashion) many methods for choosing the ensemble weights have been investigated. Here we are investigating an online classifier setting. Online learning is refered to when data becomes available in a sequential order and is used to update our predictor for future data at each step, as opposed to batch learning techniques which generate the best predictor by learning on the entire training data set at once. An example use case of online training is in internet advertising use cases when the algorithm needs to update as data regarding your seach and click pattern has been observed to produce the best next recommendation that will increase your likelihood on clicking on the advertisement. 

In this setting not only the 'weak' algorithms need to be updated. The ensemble algorithm needs to be updated with new observations. Thus the challenge here is that the ensemble method should yield a superior result in every update.

# A Bayesian Method for Optimizing Online Ensemble Weights

Now we will review the Bayesian method proposed for optimizing the ensemble weights, the subject of the paper. First, define the following:

\begin{align}
& \left\{ \left(\mathbf{x}^{t}, y^t \right) \right\} && \text{ training samples at time } t \nonumber \\
& \left\{ c^{t}_{i}\left(\mathbf{x}\right) \right\}_{i=1}^{m} && \text{ set of } m \text{ classifiers in ensemble at } t \nonumber \\
& \mathbf{\lambda} && \text{ vector of ensemble weights, } \lambda_i > 0 \nonumber \\ \nonumber \\
& \mathbf{g} = \left(g_1, \ldots, g_m \right) && \text{ vector of losses for each classifier, } g_i \geq 0 \nonumber \\
& \mathbf{g}^t = \left(g^{t}_1, \ldots, g^{t}_m \right) && \text{ vector of losses at } t \nonumber \\
& \mathbf{g}^{1:t} = \left(\mathbf{g}^{1:t}_1, \ldots, \mathbf{g}^{1:t}_m \right) && \text{ matrix of losses up to } t \nonumber \\
& l \left(\mathbf{\lambda}, \mathbf{g}^{t} \right) && \text{ loss function of the ensemble} \nonumber \\ \nonumber \\
& p\left(\mathbf{g} \big| \mathbf{\lambda} \right) = e^{-l\left(\mathbf{\lambda}, \mathbf{g}\right)} && \text{likelihood function for the vector of losses} \nonumber \\
& p\left(\mathbf{\lambda}\right) = e^{-l_{0}\left(\mathbf{\lambda}\right)} && \text{prior for the ensemble weight vector} \nonumber
\end{align}

Our objective is to choose the ensemble weight vector $\mathbf{\lambda}$ to minimize the expected value of the ensemble loss function, $E\left[l \left(\mathbf{\lambda}, \mathbf{g}^{t} \right)\right]$, which we estimate by $L_t \left(\mathbf{\lambda}; \mathbf{g}^{t} \right) = l_{0}\left(\mathbf{\lambda}\right) + \sum_{i=1}^{t} l \left(\mathbf{\lambda}; \mathbf{g}^{i} \right)$, where $l_0\left(\mathbf{\lambda}\right)$ is some initial loss. The authors suppose a number of technical regularity conditions on the function $L_t$ that are required to show later results, the most important of which is that the posterior distribution of $\mathbf{\lambda}$ will have a mean near the optimal minimum of the empirical loss, $\sum_{i=1}^{t} l \left(\mathbf{\lambda}, \mathbf{g}^{i} \right)$. This is the central result of the paper, and suggests if we could derive an appropriate posterior, we could use it to optimally update the weights.

Specifically, the posterior of interest is one that considers all the losses of each classifier up to the present time (say $t$): $p\left(\mathbf{\lambda} \big| \mathbf{g}^{1:t}\right)$. By assumed independence of the losses in each time period, we can write:

\begin{align}
p\left(\mathbf{\lambda} \big| \mathbf{g}^{1:t}\right) \propto \prod_{j=1}^{t} p\left(\mathbf{g}^{j} \big| \mathbf{\lambda}\right) p\left(\mathbf{\lambda}\right)
\end{align}

...which gives the distribution whose posterior mean will be used to update the ensemble weights, given appropriate distributional forms for the likelihood and the prior.

This setup admits the following algorithm:

* Setting: Suppose we have a set of $m$ online classifiers in an ensemble, $\left\{ c^{t}_{i}\left(\mathbf{x}\right) \right\}_{i=1}^{m}$, which we denote with $t$ to indicate they have been trained with all the data through time $t$. We are streaming new samples $\left\{ \left(\mathbf{x}^{t}, y^t \right) \right\}$, $t = 1, \ldots, T$, where $\mathbf{x}^{t}$ is the vector of available features and $y^t$ is the class label of the observation. We have set an appropriate likelihood function for the losses, $p\left(\mathbf{g}\big|\mathbf{\lambda}\right)$, and have chosen a prior, $p\left(\mathbf{\lambda}\right)$. We are interested in predicting the class labels of observations as they arrive.
* For $t = 1, \ldots, T$:
    1. For each classifier in the ensemble, compute $g_{i}^{t}$, which is a function of the decison of classifier $i$ and the true label of the observation; that is: $g_{i}^{t} = g\left(c_{i}^{t}\left(\mathbf{x}^t\right), y^t\right)$.
    2. Use this value of the loss function $g$ to update the posterior distribution of each $\lambda_i$ with its posterior mean. This distribution will depend on all loss function values observed through time $t$, such that the historical performance of every estimator in the ensemble is factored into its weight.
    3. Update the classifiers in the ensemble using the just-observed sample pair $\left(\mathbf{x}^{t}, y^t \right)$.
    4. Predict the next incoming sample by the $argmin$ of the weighted sum of the loss functions, using the most recently updated ensemble weights.

The aim of the algorithm is to recursively approximate the optimal ensemble weight vector $\mathbf{\lambda}$ that minimizes the loss function of the ensemble.

## A Specific Formulation

Now we will consider an example of the Bayesian algorithm proposed with a specific form for the ensemble loss function. This specific formulation will be used in the empirical testing to follow, as it is the paper.

In particular, suppose that the loss function of the ensemble has the form:

\begin{align}
l\left(\mathbf{\lambda}, \mathbf{g}\right) = \theta\sum_{i=1}^{m}\lambda_i g_i - \sum_{i=1}^{m}log\lambda_i \nonumber
\end{align}

...where $g$ is the logistic loss function. This definition is straightforward and is a simple weighted sum of the values of the loss function for each classifier, with an extra term to avoid the trivial minimizer of $\mathbf{\lambda} = \mathbf{0}$ (the second term shrinks rapidly as the values of each $\lambda_i$ become small). The parameter $\theta$ controls the tradeoff between minimizing the first term and maximizing the second, and effectively controls our tolerance for small weights. $\theta$ is cross-validated in the empirical section, and in practice, does not seem especially important.

If we subtract an extra term $mlog\theta$ from the ensemble loss function given (which does not affect the optimization process), then following the definition $p\left(\mathbf{g} \big| \mathbf{\lambda} \right) = e^{-l\left(\mathbf{\lambda}, \mathbf{g}\right)}$ given in the prior section, the likelihood function takes the form:

\begin{align}
p\left(\mathbf{g} \big| \mathbf{\lambda}\right) & = \text{exp} \left\{ -\theta\sum_{i=1}^{m}\lambda_{i}g_{i} + \sum_{i=1}^{m}log\lambda_i + mlog\theta \right\} \nonumber \\
& =\theta^m\left[\prod_{i=1}^{m} e^{-\theta\lambda_{i}g_{i}}\right]\left[\prod_{i=1}^{m}\lambda_i\right] \nonumber \\
& = \prod_{i=1}^{m} \theta \lambda_i e^{-\theta\lambda{i}g_i}
\end{align}

...which is the product of $m$ exponential densities, each with rate parameter $\theta \lambda_i$. Hence under this ensemble loss function, the loss of each classifier given $\lambda_i$ is exponentially distributed.

Choose a gamma prior for $\mathbf{\lambda}$ for conjugacy, $\lambda_i \sim \Gamma\left(\alpha, \beta\right)$ (where $\beta$ is a rate parameter):

\begin{align}
p\left(\mathbf{\lambda}\right) \propto \prod_{i=1}^{m} \lambda_{i}^{\alpha - 1}e^{-\beta\lambda_i}
\end{align}

From here, we are interested in deriving the posterior distribution of $\mathbf{\lambda}$ after we observed vectors of losses $\mathbf{g}$ in $t$ time periods, such that all loss information for each classifier in the ensemble is incorporated into the posterior distribution. If we choose the form of the initial loss to roughly follow that of $l\left(\mathbf{\lambda}, \mathbf{g}\right)$, then we can can set it to be:

\begin{align}
l_{0}\left(\mathbf{\lambda}\right) = \beta\sum_{i=1}^{m}\lambda_i - \left(\alpha - 1 \right)\sum_{i=1}^{m}log\lambda_i
\end{align}

(This choice is not supported by theory in the paper and by all appearances seems to be chosen so that the posterior takes a convenient distributional form. The authors do state that this term can feasibly be omitted early in the paper, so it is likely that this form is just chosen for convenience.)

With this choice of the initial loss, the ensemble loss function has the form:

\begin{align}
L_t \left(\mathbf{\lambda}; \mathbf{g}^{t} \right) & = l_{0}\left(\mathbf{\lambda}\right) + \sum_{j=1}^{t} l \left(\mathbf{\lambda}; \mathbf{g}^{j} \right) \nonumber \\
& = \beta\sum_{i=1}^{m}\lambda_i - \left(\alpha - 1 \right)\sum_{i=1}^{m}log\lambda_i + \sum_{j=1}^{t}\left[ \theta\sum_{i=1}^{m}\lambda_i g_i - \sum_{i=1}^{m}log\lambda_i \right]
\end{align}

...and hence, the posterior distribution for $\mathbf{\lambda}$ of interest is given by:

\begin{align}
p\left(\mathbf{\lambda} \big| \mathbf{g}^{1:t}\right) & = \text{exp} \left\{ L_t \left(\mathbf{\lambda}; \mathbf{g}^{t} \right) \right\} \nonumber \\
& = \text{exp} \left\{ \beta\sum_{i=1}^{m}\lambda_i - \left(\alpha - 1 \right)\sum_{i=1}^{m}log\lambda_i + \sum_{j=1}^{t}\left[ \theta\sum_{i=1}^{m}\lambda_i g_i - \sum_{i=1}^{m}log\lambda_i \right] \right \} \nonumber \\
& = \prod_{i=1}^{m} e^{-\beta \lambda_i} \lambda_{i}^{\alpha - 1}\lambda_{i}^{t} e^{\theta\lambda_i\sum_{j=1}^{t}g_{i}^{j}} \nonumber \\
& = \prod_{i=1}^{m} \lambda_{i}^{(t + \alpha) - 1} e^{-\left(\beta + \theta\sum_{j=1}^{t}g_{i}^{j}\right)\lambda_{i}}
\end{align}

...which is the product of $m$ independent $\Gamma\left(\alpha + t, \beta + \theta\sum_{j=1}^{t}g_{i}^{j} \right)$ densities.

As an aside, although it is not given in the paper, perhaps a more readily intuitive derivation of this distribution (and one that does not involve the seemingly arbitrary choice of $l_0$) simply uses the general form of the posterior from Equation (1) and uses conjugacy, from which we have:

\begin{align}
p\left(\mathbf{\lambda} \big| \mathbf{g}^{1:t}\right) & \propto \prod_{j=1}^{t} p\left(\mathbf{g}^{j} \big| \mathbf{\lambda}\right) p\left(\mathbf{\lambda}\right) \nonumber \\
& \propto \prod_{i=1}^{m}\lambda_{i}^{\alpha-1}e^{-\beta \lambda_i} \prod_{j=1}^{t}\prod_{i=1}^{m} \theta \lambda_i e^{-\theta\lambda_i g_{i}^{j}} \nonumber \\
& = \prod_{i=1}^{m} \lambda_{i}^{\alpha-1}e^{-\beta\lambda_i}\lambda_{i}^{t}e^{-\theta\lambda_{i} \sum_{j=1}^{t} g_{i}^{j}} \nonumber \\
& = \prod_{i=1}^{m} \lambda_{i}^{(t + \alpha) - 1} e^{-\left(\beta + \theta\sum_{j=1}^{t}g_{i}^{j}\right)\lambda_{i}}
\end{align}

...which is the same result as above.

 In any case, with the posterior distribution in hand, following the discussion in the prior section, the update for each ensemble weight $\lambda_i$ at each time $t$ is the posterior mean of thes distributions, given by:

\begin{align}
\frac{t + \alpha}{\beta + \theta\sum_{j=1}^{t}g_{i}^{j}}
\end{align}

This update is intuitive. Because $\alpha$, $\beta$, and $\theta$ are all positive and $t$ is growing linearly over time, it is easy to see that the weight of each classifier in the ensemble depends inversely on its historical performance in terms of the sum of its historical losses on each incoming sample. Classifiers that have historically performed poorly will have their predictions appropriately downweighted (since $\sum_{j=1}^{t}g_{i}^{j}$ will be relatively large compared to the other classifiers), while the converse will be true for strong performers. Note that all information up to the current time is incorporated into each estimate.

With the update for each ensemble weight in hand, given an incoming feature vector $\mathbf{x}$, predictions can be made using the following prediction rule:

\begin{align}
y^* = \underset{k}{argmin} \sum_{i=1}^{m} \lambda_i g_{i}\left(\mathbf{x}, k \right)
\end{align}

...which states that for $y$ we predict the class $k$ that produces the minimum sum of the weighted losses of each classifier.

This completes the derivations of the algorithm as stated in the paper. We will now use this particular form of the algorithm to run experiments on benchmark datasets.

# Empirical

This section reproduces experiments performed in the Section 5 of the paper ("Experiments") on test datasets to compare the Bayesian method proposed to other ensembling techniques. Our results match those of the authors closely. Data utilized were from five benchmark classification datasets ("heart", "mushrooms", "australian", "ionosphere", and "sonar") available here: https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html. All datasets present a binary classification problem. Full (Python) code for the experiment can be found in the appendix. (Note that the datasets as downloaded from the website are in sparse matrix format, which are most easily read using the scikit-learn function sklearn.datasets.load_svmlight_file.)

Each experiment runs as follows: 

For each trial, the test dataset in question is randomly split into an initial training portion (we use 10% of the available data, as is used in the paper) and a testing portion (the remaining 90% of the data) to be processed a single sample at a time in an online fashion. A single baseline naive bayes classifier that uses all the features is created, and a single ensemble of one-hundred naive bayes classifiers is created. To ensure differentiation in the classifiers in the ensemble, each is created to use a random subset of half of the available features in the data. The authors do not state in their paper how many features are used for their ensemble classifiers, and the number of features between datasets differs widely. This may be a source of small differences in results, though it does not seem to make a significant difference. This ensemble is then shared among the three ensembling methods considered:

1. "Voting" uses soft voting (class chosen by $argmax$ of the sums of the probability estimates for each class from each classifier in the ensemble).
2. "SGD" estimates the ensemble weights by stochastic gradient descent.
3. "Bayes" estimates the ensemble weights by the method described in this paper and make the prediction by the $argmin$ of the sums of the weighted losses for each class. Hyperparameters are set at $\alpha = 1$, $\beta = 1$, and $\theta = .1$. These are the values used in the final analysis given in the paper, and the authors show that the results of the experiments are practically invariant to the values of the hyperparameters. We present our own cross-validation for $\theta$ and $\beta$ in the Empirical section.

In brief, the outline of the experiment is given below:

* Initialize: one baseline classifier, one ensemble
* For trials $i = 1, \ldots, T$:
    1. Split the dataset into initial training (10%) and incoming testing (90%). Suppose the testing data has $N$ samples
    2. Train all classifiers on the original data. Classifiers in the ensemble are trained on their particular subset of the features
    3. For samples $(\mathbf{X}_j, y_j)$, $j = 1, \ldots, N$:
        * Make a prediction for $y_j$ using the features $\mathbf{X}_j$ for each method
        * Reveal $y_j$ and partially fit each classifier on the new sample. Record cumulative error rate up to time $j$
    4. Calculate the overall error rate on the incoming samples and store cumulative error rates as time progresses

Three main conclusions are evident:

* Ensembling methods do not always appear warranted, and often do not improve significantly upon the single online classifier that uses all the features. This is especially true in datasets where most of the features seem to be relevant (e.g., the "heart" dataset), as the performance of the ensembling classifiers that use only a subset of them will suffer significantly in this environment. The uniform voting scheme performance deteriorates rapidly in this setting, since it has no way of weighting the classifiers in the ensemble that are performing well relative to the others. This is to be expected; the ensembling methods in use are completely contrived and require no new information or assumptions about the data to function, so we should not expect them to universally improve upon a single classifier. It does seem to be true, though, that the error rates on each trial are much more consistent for the ensemble methods (that is, they are less variable) than they are for the single classifier (the single classifier can often perform very well or very poorly relative to the usual performance for the given dataset, depending upon the trial). This, again, is to be expected, and is indeed one of the main benefits of ensembling.
* The Bayesian method almost uniformly outperforms both the uniform voting ensemble and the stochastic gradient descent method for estimating the ensemble weights on each trial and each dataset. It also seems to be a somewhat stable trend among the cumulative error rates (at least on the datasets where ensembling seems to make a difference) that the Bayesian method reaches its minimum error rate earlier in the online training stream than do the other methods. This is consistent with results presented in the paper that show the Bayesian method, under certain regularity conditions, is guaranteed to converge to a global minimum in the ensemble loss function, an attractive quality that neither voting nor SGD possesses.
* The Bayesian method seems to perform particularly strongly in high-dimensional datasets, functioning as a sort of feature-selection process for weighting the classifiers in the ensemble that contain the important features. This is evident in the "mushrooms" dataset, which contains 112 features, and on which the Bayesian ensemble consistently improves upon the single baseline classifier by about a percentage point, but the uniform voting ensemble consistently underperforms it by about the same margin. This appears to be because the subsetting of features harms the ensemble significantly when it cannot reweight itself towards the classifiers that are based upon the more important features.


## Code

The R code that produces the following plots is given below. These functions are not shown when they are called for presentation purposes. Note that this code simply plots results; the results of the experiments themselves are produced by the Python code in the Appendix.

```{r}

suppressPackageStartupMessages(library(data.table))
suppressPackageStartupMessages(library(ggplot2))
suppressPackageStartupMessages(library(knitr))
suppressPackageStartupMessages(library(scales))
suppressPackageStartupMessages(library(stringr))

opts_chunk$set(echo = FALSE)

root <- '/Users/zgallegos/documents/school/math_538/project/data/results/'

acc_path <- paste0(root, 'accuracy/')
cum_path <- paste0(root, 'cumulative/')
loss_path <- paste0(root, 'loss/')
cval_theta_path <- paste0(root, 'cval_theta/')
cval_beta_path <- paste0(root, 'cval_beta/')

read <- function(x, id)
{
    d <- read.csv(x, header = TRUE)
    d$X <- NULL
    melt(d, id = id, value.name = 'error', variable.name = 'Method')
}

err_plot <- function(dataset)
{

    fl <- paste0(acc_path, dataset, '_errors.csv')

    data <- read(fl, 'trial')

    if(dataset == 'mushrooms')
        data <- data[data$Method != 'SGD.Weighting',]

    proper <- gsub('(?<=\\b)([a-z])', '\\U\\1', tolower(dataset), perl = TRUE)
    ttl <- paste0(proper, ' - Error Rates By Method/Trial')

    y_lower <- .975 * min(data$error)
    y_max <- 1.025 * max(data$error)

    data$trial <- data$trial + 1

    g <- ggplot(data = data,
            aes(x = as.factor(trial), y = error, fill = Method, group = Method))
    g <- g + geom_bar(stat = 'identity', position = 'dodge')
    g <- g + coord_cartesian(ylim = c(y_lower, y_max))
    g <- g + theme_bw() + theme(text = element_text(family = 'serif'))
    g <- g + ggtitle(ttl) + xlab('Trial') + ylab('Error')
    g

}

cum_plot <- function(dataset, trial)
{

    fl <- paste0(cum_path, dataset, '_cumerrors_trial_', trial, '.csv')
    data <- read(fl, 'index')

    if(dataset == 'mushrooms')
        data <- data[data$Method != 'SGD.Weighting',]

    proper <- gsub('(?<=\\b)([a-z])', '\\U\\1', tolower(dataset), perl = TRUE)
    ttl <- paste0(proper, ' - Cumulative Error, Trial ', trial + 1)

    g <- ggplot(data = data, aes(x = index, y = error, colour = Method))
    g <- g + geom_line()
    g <- g + coord_cartesian(xlim = c(20, nrow(data) / length(unique(data$Method))))
    g <- g + theme_bw() + theme(text = element_text(family = 'serif'))
    g <- g + ggtitle(ttl) + xlab('Sample') + ylab('Cumulative Error')
    g

}

loss_plot <- function(dataset, trial)
{

    proper <- gsub('(?<=\\b)([a-z])', '\\U\\1', tolower(dataset), perl = TRUE)
    ttl <- paste0(proper, ' Cumulative Errors and Ensemble Weights ',
                  'for Selected Ensemble Classifiers, Trial ', trial + 1)

    loss_fl <- paste0(loss_path, dataset, '_loss_samp_', trial, '.csv')
    lam_fl <- paste0(loss_path, dataset, '_lam_samp_', trial, '.csv')

    ind <- 1:4

    loss <- read.csv(loss_fl)
    for(i in ind[-1]) loss[,i] <- cumsum(loss[,i])
    loss$what <- 'Cumulative Loss'

    lam <- read.csv(lam_fl)
    lam$what <- 'Ensemble Weight'

    names(loss)[ind] <- names(lam)[ind] <- c('Sample', 'Worst', 'Median', 'Best')

    loss <- melt(loss, id = c('Sample', 'what'), variable.name = 'Classifier')
    lam <- melt(lam, id = c('Sample', 'what'), variable.name = 'Classifier')

    plt <- rbind(lam, loss)

    plt$what <- factor(plt$what, 
                        levels = c('Ensemble Weight', 'Cumulative Loss'),
                        ordered = TRUE)
    
    plt$Classifier <- factor(plt$Classifier,
                        levels = c('Best', 'Median', 'Worst'),
                        ordered = TRUE)

    g <- ggplot(data = plt, aes(x = Sample, y = value, colour = Classifier))
    g <- g + geom_line() + facet_grid(what ~ ., scales = 'free')
    g <- g + ggtitle(ttl)
    g <- g + theme_bw() + theme(text = element_text(family = 'serif'),
                                axis.title.y = element_blank())
    g

}

select_feats <- function(dataset, nmz, n)
{

    fl <- function(k) paste0(loss_path, dataset, '_feats_samp_', k, '.csv')

    best_feats <- vector('list', n)

    for(i in 0:(n-1)) {

        best_feats[[i+1]] <- read.csv(fl(i))[, 'loss_max']

    }

    tbl <- as.data.frame(table(Reduce(c, best_feats)))
    tbl <- tbl[order(-tbl$Freq),]
    tbl$Var1 <- as.numeric(tbl$Var1)

    tbl$Var <- unlist(lapply(tbl$Var1, function(i) nmz[i+1]))

    names(tbl) <- c('', 'Appearances in Strongest Classifier', 'Variable')

    tbl[, c(3, 2)]

}

cval_plot <- function(type)
{

    if(type == 'theta') {
        fl <- paste0(cval_theta_path, 'cval_theta.csv')
    } 
    else if(type == 'beta') {
        fl <- paste0(cval_beta_path, 'cval_beta.csv')
    }

    data <- read.csv(fl, header = TRUE)

    data$Dataset <- gsub('(?<=\\b)([a-z])', '\\U\\1', 
                            tolower(data$Dataset), 
                            perl = TRUE)

    if(type == 'theta') {
        data$xvar <- data$Theta
        xlb <- expression(theta)
        ttl <- 'Theta'
    }
    else if(type == 'beta') {
        data$xvar <- data$Beta
        xlb <- expression(beta)
        ttl <- 'Beta'
    }

    g <- ggplot(data = data, 
            aes(x = xvar, y = Mean.Error, colour = Dataset, group = Dataset))
    g <- g + geom_line() + geom_point()
    g <- g + ggtitle(paste0('Mean Error Rates by Value of ', ttl, ' - Five Trials'))
    g <- g + xlab(xlb)
    g <- g + theme_bw() + theme(text = element_text(family = 'serif'))
    g

}

mns <- function(dataset)
{

    fl <- paste0(acc_path, dataset, '_errors.csv')
    proper <- gsub('(?<=\\b)([a-z])', '\\U\\1', tolower(dataset), perl = TRUE)

    data <- as.data.table(read(fl, 'trial'))
    data$Dataset <- proper

    as.data.frame(data[, .(`Mean Error` = mean(error)), by = .(Dataset, Method)])

}


```

## Average Error Rate by Method

The table below shows the average error rate for incoming one-step classifications made by each classifier at the end of the experiment. Ten trials were performed for each dataset.

Note that SGD was not performed for the "mushrooms" dataset due to time constraints (because of its relatively large size and high number of features, each trial of SGD weighting for this dataset took roughly three hours).

```{r}

setwd('/Users/zgallegos/documents/school/math_538/project/data/')

datasets <- c('australian', 'ionosphere', 'heart', 'sonar', 'mushrooms')

tbl <- mns(datasets[1])

for(i in datasets[-1]) tbl <- rbind(tbl, mns(i))

tbl <- dcast(tbl, Dataset ~ Method, value.var = 'Mean Error')

tbl$imp <- unlist(lapply(seq(nrow(tbl)), function(i) {
            b <- min(tbl[i, setdiff(names(tbl), c('Dataset', 'Bayesian.Weighting'))])
            percent((b - tbl[i, 'Bayesian.Weighting']) / b)
            }))

for(i in 2:5) {
     tbl[,i] <- round(tbl[,i], 5)
     tbl[,i] <- str_pad(tbl[,i], width = 7, side = 'right', pad = '0')
 }

tbl$`SGD.Weighting`[grep('^100', tbl$`SGD.Weighting`)] <- '-'

names(tbl) <- c('Dataset', 'Single Classifier', 'Voting Ensemble',
                'Bayesian Weighting', 'SGD Weighting', 'Bayes +- Best Other')

kable(tbl, align = 'c')

```

Bayesian ensemble weighting produces the lowest error rate in all but one dataset ("heart") and universally improves upon simple voting and SGD weighting, sometimes by a relatively strong amount (e.g., for the "mushrooms" dataset, the improvement upon the second best method roughly 29%).

The success of the method seems dependent on the nature of the features in the dataset. It seems to perform most strongly in datasets where the number of features is high, and likely contains many features that are weak predictors of the observation class. In this setting, the ability of the algorithm to re-weight the ensemble towards the classifiers that contain the more important predictors is very important, whereas it may not be as important in smaller datasets where most of the features contain some predictive power (e.g., "heart").

## Error Rates by Method and Trial

To inform our intuition of how consistent the performance is for each method across different random orderings of the dataset, error rates by method for each trial and method are given below.


```{r}

err_plot('australian')

```

```{r}

err_plot('ionosphere')

```

```{r}

err_plot('heart')

```

```{r}

err_plot('mushrooms')

```

```{r}

err_plot('sonar')

```

The ordering of the dataset does seem to be somewhat important, especially for the particular datasets where the ending accuracy of each method is relatively close. The exception is the "mushrooms" dataset, where the Bayesian method unambiguously outperforms all the single classifier and the simple voting scheme by a large margin on every trial. We will explain the reason for this shortly.

## Bayesian Ensemble Weights and Cumulative Loss

Some guidance about how important the Bayesian re-weighting is for a given dataset can be drawn from observing how the cumulative losses of the classifiers and their ensemble weights tend to move as the stream of incoming data progresses. Below, we plot for each dataset and a selected trial the cumulative loss and ensemble weight of three classifiers in the ensemble: the one with the highest ending weight at the end of the experiment, the smallest, and the median.


```{r}

loss_plot('australian', 0)


```

```{r}

loss_plot('ionosphere', 0)


```

```{r}

loss_plot('heart', 0)


```

```{r}

loss_plot('mushrooms', 0)


```

```{r}

loss_plot('sonar', 0)


```

Observe that for the datasets where the Bayesian method produces only a small improvement in accuracy (or does not improve it at all) that the ensemble weights tend to stay closer together, suggesting that there is not much variation between the classifiers in the ensemble in terms of predictive power. In this setting, we would expect the single classifier that uses all the features to provide a good approximation to the limit of performance for the dataset. In contrast, for the "mushrooms" dataset, the strongest classifier has an ending ensemble weight roughly ten times that of the weakest classifier (186.44 compared to 17.57) and over four times that of the median classifier (186.44 compared to 43.89), and this is the dataset where the strongest performance increase is produced. Thus, it is clear that the Bayesian method flourishes in high-dimensional spaces where it acts as a sort of feature selection process, reorienting the predictions toward that are the classifiers that are performing the strongest.


## Cumulative Error Rates

The cumulative error as the streaming samples progress for each method on a selected trial is given below:


```{r}

cum_plot('australian', 2)

```

```{r}

cum_plot('ionosphere', 3)

```

```{r}

cum_plot('heart', 1)

```

```{r}

cum_plot('mushrooms', 4)

```

```{r}

cum_plot('sonar', 0)

```

## Cross-Validation for the Ensemble Loss Parameter

Recall that the ensemble loss function employed in the theoretical formulation of the Bayesian method in this section is given by:

\begin{align}
l\left(\mathbf{\lambda}, \mathbf{g}\right) = \theta\sum_{i=1}^{m}\lambda_i g_i - \sum_{i=1}^{m}log\lambda_i \nonumber
\end{align}

...where the term $\theta$ controls the tradeoff between the weighted loss of each classifier and the term $\sum_{i=1}^{m}log\lambda_i$, which prevents $\mathbf{\lambda}$ from approaching the trivial solution of $\mathbf{0}$. In this section, we perform a brief cross-validation for this term to inform our analysis of how important this term is. Recall also that the authors of the paper found this term to be unimportant.

We specify eight equally-spaced values of $\theta$ ranging from roughly .15 to 2. For two datasets, "heart" and "australian", we perform five trials for each value of $\theta$ and record the average error rate. Results are plotted below:

```{r, fig.width = 7}

cval_plot('theta')

```

## Cross-Validation for Prior Hyperparameter Beta

Recall that the prior distribution specified for the ensemble weights were each $\Gamma\left(\alpha, \beta\right)$. Here we perform a similar cross-validation as presented in the prior section for the hyperparameter $\beta$. Note that since it will cancel in the expression for $\lambda_i$ in the prediction rule given in Equation (9), there is no need to cross-validate $\alpha$.

We specify eight equally-spaces values for $\beta$ ranging from roughly .05 to 8. Again, we use the same two datasets and perform five trials each.

```{r, fig.width = 7}

cval_plot('beta')

```

Again, we see that the mean error rate is not especially sensitive to the choice of $\beta$. This supports the conclusion in the paper that the choice of hyperparameters is not terribly consequential for the Bayesian method.

# Appendix

## Empirical Section Full Code

The results shown in the Empirical section were produced by the following Python code:

```{python, eval = FALSE}

"""
not run
"""
import pandas as pd
import numpy as np
import os
import re

from sklearn import naive_bayes
from sklearn.base import clone
from sklearn.model_selection import train_test_split
from sklearn.metrics import log_loss, accuracy_score
from sklearn.datasets import load_svmlight_file
from sklearn.linear_model import SGDClassifier


def create_ensemble(classifier, X, num, num_features):
    """
    create a uniform ensemble of <num> classifiers, each of which uses
    <num_features> randomly-sampled features of the data
    """
    estimators = [None] * num
    n_feats = X.shape[1]

    for i in range(num):

        estimators[i] = clone(classifier)
        estimators[i].sub_feats = list(np.sort(np.random.choice(
            range(n_feats), size = num_features, replace = False)))

    return estimators


def fit_ensemble(ensemble, x, y, partial = False, classes = None):
    """
    fit the ensemble, each classifier with its own set of features
    if <partial>, do an online fit
    """
    for e in ensemble:

        x_ = x[:, e.sub_feats]
        
        if not partial:

            e.fit(x_, y)

        else:

            e.partial_fit(x_, y, classes = classes)

    return ensemble


def fit_sgd_ensemble(ensemble, X, y):
    sgd = SGDClassifier(loss='log')
    ensemble_prediction = []
    for e in ensemble:

        X_ = X[:, e.sub_feats]

        e.fit(X_, y)
        ensemble_prediction.append(e.predict_proba(X_)[:,1])
    return sgd.fit(np.array(ensemble_prediction).transpose(),y)


def predict_sgd(ensemble, fitted_sgd, X):
    """
    """
    ensemble_preds = [e.predict_proba(X[:, e.sub_feats])[:,1][0] for e in ensemble]
    return fitted_sgd.predict(np.array(ensemble_preds).reshape(1,-1))


def update_bayes(ensemble, i, x, y, classes, a, b, theta):
    """
    update loss and ensemble weights
    """
    t = i + 1

    for e in ensemble:

        x_ = x[:, e.sub_feats]
        pred = e.predict_proba(x_)
        e.loss[i] = log_loss(y, pred, labels = classes)

        num = a + t
        den = b + theta * np.sum(e.loss[0:t])
        e.lam[i] = num / den

    return ensemble


def predict_bayes(ensemble, i, X, classes):
    """
    ensemble prediction using posterior ensemble weights
    """
    m = len(ensemble)
    neg_label, pos_label = classes
    
    if i == -1:

        return predict_vote(ensemble, X, classes)

    curr_lams = np.array([e.lam[i] for e in ensemble])

    loss_neg = [None] * m
    loss_pos = [None] * m

    for i, e in enumerate(ensemble):

        pred = e.predict_proba(X[:, e.sub_feats])

        loss_neg[i] = log_loss([neg_label], pred, labels = classes)
        loss_pos[i] = log_loss([pos_label], pred, labels = classes)

    loss_neg = np.array(loss_neg)
    loss_pos = np.array(loss_pos)

    probs = [sum(curr_lams * loss_neg), sum(curr_lams * loss_pos)]

    return classes[np.argmin(probs)]


def predict_vote(ensemble, X, classes):
    """
    ensemble prediction by soft voting
    """
    ensemble_preds = [e.predict_proba(X[:, e.sub_feats]) for e in ensemble]
    pred = np.concatenate(ensemble_preds).sum(axis = 0)

    return classes[np.argmax(pred)]


def output_loss(ensemble, dataset, trial):
    """
    """
    end_lams = np.array([e.lam[-1] for e in ensemble])

    min_ind = np.argmin(end_lams)
    max_ind = np.argmax(end_lams)
    med_ind = np.argsort(end_lams)[len(end_lams) // 2]

    lams_towrite = np.stack((ensemble[min_ind].lam,
                             ensemble[med_ind].lam,
                             ensemble[max_ind].lam), axis = 1)

    loss_towrite = np.stack((ensemble[min_ind].loss,
                             ensemble[med_ind].loss,
                             ensemble[max_ind].loss), axis = 1)

    feats_towrite = np.stack((ensemble[min_ind].sub_feats,
                              ensemble[med_ind].sub_feats,
                              ensemble[max_ind].sub_feats), axis = 1)

    df = pd.DataFrame(lams_towrite, columns = ['lam_min', 'lam_med', 'lam_max'])
    df.to_csv(os.path.join('results', 'loss', dataset + '_lam_samp_' + str(trial) + '.csv'))

    df = pd.DataFrame(loss_towrite, columns = ['loss_min', 'loss_med', 'loss_max'])
    df.to_csv(os.path.join('results', 'loss', dataset + '_loss_samp_' + str(trial) + '.csv'))

    df = pd.DataFrame(feats_towrite, columns = ['loss_min', 'loss_med', 'loss_max'])
    df.to_csv(os.path.join('results', 'loss', dataset + '_feats_samp_' + str(trial) + '.csv'))


def online_test(ensemble, single, X, y, classes, start_prop, do_sgd, **kwargs):
    """
    perform the tests
    """
    X_start, X_stream, y_start, y_stream = train_test_split(
        X, y, test_size = 1 - start_prop)

    stream_size = X_stream.shape[0]

    single_preds = [None] * stream_size
    vote_preds = [None] * stream_size
    bayes_preds = [None] * stream_size

    if do_sgd:
    
        sgd_preds = [None] * stream_size

    else:

        sgd_preds = [0] * stream_size

    for e in ensemble:

        e.loss = [None] * stream_size
        e.lam = [None] * stream_size


    """
    initial fit
    """

    single.fit(X_start, y_start)
    ensemble = fit_ensemble(ensemble, X_start, y_start)

    if do_sgd:

        fitted_sgd = (fit_sgd_ensemble(ensemble, X_start, y_start))

    _X_ = X_start
    _y_ = y_start

    for j in range(stream_size):

        x_ = X_stream[j].reshape(1, -1)
        y_ = y_stream[j].reshape(1,)

        """
        update bayes loss and weights
        """

        ensemble = update_bayes(ensemble, j, x_, y_, classes, **kwargs)

        """
        make predictions
        """

        single_preds[j] = single.predict(x_)
        vote_preds[j] = predict_vote(ensemble, x_, classes)
        bayes_preds[j] = predict_bayes(ensemble, j - 1, x_, classes)

        if do_sgd:

            sgd_preds[j] = predict_sgd(ensemble, fitted_sgd, x_)[0]

        """
        partial fit
        """

        single.partial_fit(x_, y_, classes = classes)
        ensemble = fit_ensemble(ensemble, x_, y_, partial = True, classes = classes)

        _X_=(np.vstack((_X_,x_)))
        _y_=(np.append(_y_,y_))

        if do_sgd:

            fitted_sgd = fit_sgd_ensemble(ensemble, _X_, _y_)

    single_preds = np.array(single_preds).reshape(y_stream.shape)
    vote_preds = np.array(vote_preds).reshape(y_stream.shape)
    bayes_preds = np.array(bayes_preds).reshape(y_stream.shape)
    sgd_preds = np.array(sgd_preds).reshape(y_stream.shape)

    return (ensemble, single_preds, vote_preds, 
            bayes_preds, sgd_preds, y_stream)




if __name__ == '__main__':


    os.chdir('/users/zgallegos/documents/school/math_538/project/data')


    """
    """

    """
    run the main tests. five data files, ten trials each
    """

    use_datasets = ['heart.txt', 'australian.txt', 'ionosphere.txt', 
                    'sonar.txt', 'mushrooms.txt']
    use_classes = [(-1, 1), (-1, 1), (-1, 1), (-1, 1), (1, 2)]

    n_trials = 10
    feat_perc = .5 # percentage of the features ensemble classifiers get

    for j, k in enumerate(use_datasets):

        if k == 'mushrooms.txt':

            do_sgd = False

        else:

            do_sgd = True
        
        fl_name = re.search('^.+?(?=\.txt)', k).group(0)
        err_fl = fl_name + '_errors.csv'

        cls = use_classes[j]

        X, y = load_svmlight_file(k)
        X = X.toarray()
        y = y.astype(int)

        feats = int(np.floor(feat_perc * X.shape[1]))

        sing = []
        vote = []
        bayes = []
        sgd = []

        trlz = range(n_trials)

        for i in trlz:

            cumerr_fl = fl_name + '_cumerrors_trial_%s.csv' % i

            single = naive_bayes.BernoulliNB()
            ensemble = create_ensemble(naive_bayes.BernoulliNB(), X, 100, feats)

            e, s, v, b, d, y_ = online_test(ensemble, single, X, y, cls, .1, do_sgd, 
                                                a = 1, b = 1, theta = .1)

            indx = range(1, len(y_) + 1)
            cum_sing = 1 - np.cumsum(s == y_) / indx
            cum_vote = 1 - np.cumsum(v == y_) / indx
            cum_bayes = 1 - np.cumsum(b == y_) / indx
            cum_sgd = 1 - np.cumsum(d == y_) / indx

            to_write = np.stack((indx, cum_sing, cum_vote, cum_bayes, cum_sgd), axis = 1)
            
            df = pd.DataFrame(to_write, 
                    columns = ['index', 'Single Classifier', 'Voting', 
                               'Bayesian Weighting', 'SGD Weighting'])

            df.to_csv(os.path.join('results', 'cumulative', cumerr_fl))

            sing.append(1 - accuracy_score(y_, s))
            vote.append(1 - accuracy_score(y_, v))
            bayes.append(1 - accuracy_score(y_, b))
            sgd.append(1 - accuracy_score(y_, d))

            output_loss(e, fl_name, i)

        to_write = np.stack((trlz, sing, vote, bayes, sgd), axis = 1)

        df = pd.DataFrame(to_write, 
                    columns = ['trial', 'Single Classifier', 'Voting', 
                               'Bayesian Weighting', 'SGD Weighting'])

        df.to_csv(os.path.join('results', 'accuracy', err_fl))

    
    """
    """


    """
    cross-validate the ensemble loss parameter, theta
    """

    use_datasets = ['heart.txt', 'australian.txt']
    use_classes = [(-1, 1), (-1, 1)]

    n_trials = 5
    feat_perc = .5

    thetas = np.linspace(.015, 2, 8)

    results_data = []
    results_theta = []
    results_err = []

    for j, d in enumerate(use_datasets):

        print('doing dataset %s' % d)

        fl_name = re.search('^.+?(?=\.txt)', d).group(0)

        cls = use_classes[j]

        X, y = load_svmlight_file(d)
        X = X.toarray()
        y = y.astype(int)

        feats = int(np.floor(feat_perc * X.shape[1]))

        for t in thetas:

            print('doing theta %s' % t)

            bayes = []

            for i in range(n_trials):

                single = naive_bayes.BernoulliNB()
                ensemble = create_ensemble(naive_bayes.BernoulliNB(), X, 100, feats)

                e, s, v, b, d, y_ = online_test(ensemble, single, X, y, cls, .1, False,
                                                    a = 1, b = 1, theta = t)

                bayes.append(1 - accuracy_score(y_, b))

            results_data.append(fl_name)
            results_theta.append(t)
            results_err.append(np.mean(bayes))

            print('num trials %s, mean error rate %s' % (str(len(bayes)), str(np.mean(bayes))))

    to_write = np.stack((results_data, results_theta, results_err), axis = 1)

    df = pd.DataFrame(to_write, 
                    columns = ['Dataset', 'Theta', 'Mean Error'])

    df.to_csv(os.path.join('results', 'cval_theta', 'cval_theta.csv'))

    
    """
    """

    """
    cross validate beta
    """

    use_datasets = ['heart.txt', 'australian.txt']
    use_classes = [(-1, 1), (-1, 1)]

    n_trials = 5
    feat_perc = .5

    betas = np.linspace(.0625, 8, 8)

    results_data = []
    results_beta = []
    results_err = []

    for j, d in enumerate(use_datasets):

        print('doing dataset %s' % d)

        fl_name = re.search('^.+?(?=\.txt)', d).group(0)

        cls = use_classes[j]

        X, y = load_svmlight_file(d)
        X = X.toarray()
        y = y.astype(int)

        feats = int(np.floor(feat_perc * X.shape[1]))

        for b_ in betas:

            print('doing beta %s' % b_)

            bayes = []

            for i in range(n_trials):

                single = naive_bayes.BernoulliNB()
                ensemble = create_ensemble(naive_bayes.BernoulliNB(), X, 100, feats)

                e, s, v, b, d, y_ = online_test(ensemble, single, X, y, cls, .1, False,
                                                    a = 1, b = b_, theta = .1)

                bayes.append(1 - accuracy_score(y_, b))

            results_data.append(fl_name)
            results_beta.append(b_)
            results_err.append(np.mean(bayes))

            print('num trials %s, mean error rate %s' % (str(len(bayes)), str(np.mean(bayes))))

    to_write = np.stack((results_data, results_beta, results_err), axis = 1)

    df = pd.DataFrame(to_write, 
                    columns = ['Dataset', 'Beta', 'Mean Error'])

    df.to_csv(os.path.join('results', 'cval_beta', 'cval_beta.csv'))

    
    """
    """


```