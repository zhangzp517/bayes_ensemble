---
output:
  pdf_document:
    fig_height: 7
    fig_width: 7
author: 'Zachary Gallegos and Saman Sirafi'
title: 'A Bayesian Approach for Online Classifier Ensemble (Bai, Lam, and Sclaroff)'
abstract: 'This paper reviews a technique presented in "A Bayesian Approach for Online Classifier Ensemble" (Bai, Lam, and Sclaroff) for optimizing the selection of ensemble weights in an online classification setting. The presentation is Bayesian in fashion and involves defining a likelihood distribution for the losses of each classifier and a prior distribution for the ensemble weights, and updating the weights at each step based on their posterior distribution. The authors experiments (and the reproductions in this paper) show that this method often compares favorably to a single online classifier and other ensemble weighting schemes, such as uniform voting and stochastic gradient descent (SGD).'
---

# Introduction

In classification problems, ensembling is a popular technique in machine learning that involves aggregating many "weak" classifiers into a "strong" one that, ideally, outperforms any of the individual classifiers on its own. In the interest of optimizing the aggregation process, individual classifiers in the ensemble are often weighted in some fashion that is intended to produce more accurate predictions. In a standard offline setting (that is, one in which the model is trained once and new data is not added to the trained model in any fashion) many methods for choosing the ensemble weights have been investigated.

# A Bayesian Method for Optimizing Online Ensemble Weights

Now we will review the Bayesian method proposed for optimizing the ensemble weights, the subject of the paper. First, define the following:

\begin{align}
& \left\{ \left(\mathbf{x}^{t}, y^t \right) \right\} && \text{ training samples at time } t \nonumber \\
& \left\{ c^{t}_{i}\left(\mathbf{x}\right) \right\}_{i=1}^{m} && \text{ set of } m \text{ classifiers in ensemble at } t \nonumber \\
& \mathbf{\lambda} && \text{ vector of ensemble weights, } \lambda_i > 0 \nonumber \\ \nonumber \\
& \mathbf{g} = \left(g_1, \ldots, g_m \right) && \text{ vector of losses for each classifier, } g_i \geq 0 \nonumber \\
& \mathbf{g}^t = \left(g^{t}_1, \ldots, g^{t}_m \right) && \text{ vector of losses at } t \nonumber \\
& \mathbf{g}^{1:t} = \left(\mathbf{g}^{1:t}_1, \ldots, \mathbf{g}^{1:t}_m \right) && \text{ matrix of losses up to } t \nonumber \\
& l \left(\mathbf{\lambda}, \mathbf{g}^{t} \right) && \text{ loss function of the ensemble} \nonumber \\ \nonumber \\
& p\left(\mathbf{g} \big| \mathbf{\lambda} \right) = e^{-l\left(\mathbf{\lambda}, \mathbf{g}\right)} && \text{likelihood function for the vector of losses} \nonumber \\
& p\left(\mathbf{\lambda}\right) = e^{-l_{0}\left(\mathbf{\lambda}\right)} && \text{prior for the ensemble weight vector} \nonumber
\end{align}

Our objective is to choose the ensemble weight vector $\mathbf{\lambda}$ to minimize the expected value of the ensemble loss function, $E\left[l \left(\mathbf{\lambda}, \mathbf{g}^{t} \right)\right]$, which we estimate by $L_t \left(\mathbf{\lambda}; \mathbf{g}^{t} \right) = l_{0}\left(\mathbf{\lambda}\right) + \sum_{i=1}^{t} l \left(\mathbf{\lambda}; \mathbf{g}^{i} \right)$, where $l_0\left(\mathbf{\lambda}\right)$ is some initial loss. The authors suppose a number of technical regularity conditions on the function $L_t$ that are required to show later results, the most important of which is that the posterior distribution of $\mathbf{\lambda}$ will have a mean near the optimal minimum of the empirical loss, $\sum_{i=1}^{t} l \left(\mathbf{\lambda}, \mathbf{g}^{i} \right)$. This is the central result of the paper, and suggests if we could derive an appropriate posterior, we could use it to optimally update the weights.

Specifically, the posterior of interest is one that considers all the losses of each classifier up to the present time (say $t$): $p\left(\mathbf{\lambda} \big| \mathbf{g}^{1:t}\right)$. By assumed independence of the losses in each time period, we can write:

\begin{align}
p\left(\mathbf{\lambda} \big| \mathbf{g}^{1:t}\right) \propto \prod_{j=1}^{t} p\left(\mathbf{g}^{j} \big| \mathbf{\lambda}\right) p\left(\mathbf{\lambda}\right)
\end{align}

...which gives the distribution whose posterior mean will be used to update the ensemble weights, given appropriate distributional forms for the likelihood and the prior.

This setup admits the following algorithm:

* Setting: Suppose we have a set of $m$ online classifiers in an ensemble, $\left\{ c^{t}_{i}\left(\mathbf{x}\right) \right\}_{i=1}^{m}$, which we denote with $t$ to indicate they have been trained with all the data through time $t$. We are streaming new samples $\left\{ \left(\mathbf{x}^{t}, y^t \right) \right\}$, $t = 1, \ldots, T$, where $\mathbf{x}^{t}$ is the vector of available features and $y^t$ is the class label of the observation. We have set an appropriate likelihood function for the losses, $p\left(\mathbf{g}\big|\mathbf{\lambda}\right)$, and have chosen a prior, $p\left(\mathbf{\lambda}\right)$. We are interested in predicting the class labels of observations as they arrive.
* For $t = 1, \ldots, T$:
    1. For each classifier in the ensemble, compute $g_{i}^{t}$, which is a function of the decison of classifier $i$ and the true label of the observation; that is: $g_{i}^{t} = g\left(c_{i}^{t}\left(\mathbf{x}^t\right), y^t\right)$.
    2. Use this value of the loss function $g$ to update the posterior distribution of each $\lambda_i$ with its posterior mean. This distribution will depend on all loss function values observed through time $t$, such that the historical performance of every estimator in the ensemble is factored into its weight.
    3. Update the classifiers in the ensemble using the just-observed sample pair $\left(\mathbf{x}^{t}, y^t \right)$.
    4. Predict the next incoming sample by the $argmin$ of the weighted sum of the loss functions, using the most recently updated ensemble weights.

The aim of the algorithm is to recursively approximate the optimal ensemble weight vector $\mathbf{\lambda}$ that minimizes the loss function of the ensemble.

## A Specific Formulation

Now we will consider an example of the Bayesian algorithm proposed with a specific form for the ensemble loss function. This specific formulation will be used in the empirical testing to follow, as it is the paper.

In particular, suppose that the loss function of the ensemble has the form:

\begin{align}
l\left(\mathbf{\lambda}, \mathbf{g}\right) = \theta\sum_{i=1}^{m}\lambda_i g_i - \sum_{i=1}^{m}log\lambda_i \nonumber
\end{align}

...where $g$ is the logistic loss function. This definition is straightforward and is a simple weighted sum of the values of the loss function for each classifier, with an extra term to avoid the trivial minimizer of $\mathbf{\lambda} = \mathbf{0}$ (the second term shrinks rapidly as the values of each $\lambda_i$ become small). The parameter $\theta$ controls the tradeoff between minimizing the first term and maximizing the second, and effectively controls our tolerance for small weights (in practice, this term does not seem especially important).

If we subtract an extra term $mlog\theta$ from the ensemble loss function given (which does not affect the optimization process), then following the definition $p\left(\mathbf{g} \big| \mathbf{\lambda} \right) = e^{-l\left(\mathbf{\lambda}, \mathbf{g}\right)}$ given in the prior section, the likelihood function takes the form:

\begin{align}
p\left(\mathbf{g} \big| \mathbf{\lambda}\right) & = \text{exp} \left\{ -\theta\sum_{i=1}^{m}\lambda_{i}g_{i} + \sum_{i=1}^{m}log\lambda_i + mlog\theta \right\} \nonumber \\
& =\theta^m\left[\prod_{i=1}^{m} e^{-\theta\lambda_{i}g_{i}}\right]\left[\prod_{i=1}^{m}\lambda_i\right] \nonumber \\
& = \prod_{i=1}^{m} \theta \lambda_i e^{-\theta\lambda{i}g_i}
\end{align}

...which is the product of $m$ exponential densities, each with rate parameter $\theta \lambda_i$. Hence under this ensemble loss function, the loss of each classifier given $\lambda_i$ is exponentially distributed.

Choose a gamma prior for $\mathbf{\lambda}$ for conjugacy, $\lambda_i \sim \Gamma\left(\alpha, \beta\right)$ (where $\beta$ is a rate parameter):

\begin{align}
p\left(\mathbf{\lambda}\right) \propto \prod_{i=1}^{m} \lambda_{i}^{\alpha - 1}e^{-\beta\lambda_i}
\end{align}

From here, we are interested in deriving the posterior distribution of $\mathbf{\lambda}$ after we observed vectors of losses $\mathbf{g}$ in $t$ time periods, such that all loss information for each classifier in the ensemble is incorporated into the posterior distribution. If we choose the form of the initial loss to roughly follow that of $l\left(\mathbf{\lambda}, \mathbf{g}\right)$, then we can can set it to be:

\begin{align}
l_{0}\left(\mathbf{\lambda}\right) = \beta\sum_{i=1}^{m}\lambda_i - \left(\alpha - 1 \right)\sum_{i=1}^{m}log\lambda_i
\end{align}

(This choice is not supported by theory in the paper and by all appearances seems to be chosen so that the posterior takes a convenient distributional form. The authors do state that this term can feasibly be omitted early in the paper, so it is likely that this form is just chosen for convenience.)

With this choice of the initial loss, the ensemble loss function has the form:

\begin{align}
L_t \left(\mathbf{\lambda}; \mathbf{g}^{t} \right) & = l_{0}\left(\mathbf{\lambda}\right) + \sum_{j=1}^{t} l \left(\mathbf{\lambda}; \mathbf{g}^{j} \right) \nonumber \\
& = \beta\sum_{i=1}^{m}\lambda_i - \left(\alpha - 1 \right)\sum_{i=1}^{m}log\lambda_i + \sum_{j=1}^{t}\left[ \theta\sum_{i=1}^{m}\lambda_i g_i - \sum_{i=1}^{m}log\lambda_i \right]
\end{align}

...and hence, the posterior distribution for $\mathbf{\lambda}$ of interest is given by:

\begin{align}
p\left(\mathbf{\lambda} \big| \mathbf{g}^{1:t}\right) & = \text{exp} \left\{ L_t \left(\mathbf{\lambda}; \mathbf{g}^{t} \right) \right\} \nonumber \\
& = \text{exp} \left\{ \beta\sum_{i=1}^{m}\lambda_i - \left(\alpha - 1 \right)\sum_{i=1}^{m}log\lambda_i + \sum_{j=1}^{t}\left[ \theta\sum_{i=1}^{m}\lambda_i g_i - \sum_{i=1}^{m}log\lambda_i \right] \right \} \nonumber \\
& = \prod_{i=1}^{m} e^{-\beta \lambda_i} \lambda_{i}^{\alpha - 1}\lambda_{i}^{t} e^{\theta\lambda_i\sum_{j=1}^{t}g_{i}^{j}} \nonumber \\
& = \prod_{i=1}^{m} \lambda_{i}^{(t + \alpha) - 1} e^{-\left(\beta + \theta\sum_{j=1}^{t}g_{i}^{j}\right)\lambda_{i}}
\end{align}

...which is the product of $m$ independent $\Gamma\left(\alpha + t, \beta + \theta\sum_{j=1}^{t}g_{i}^{j} \right)$ densities.

As an aside, although it is not given in the paper, perhaps a more readily intuitive derivation of this distribution (and one that does not involve the seemingly arbitrary choice of $l_0$) simply uses the general form of the posterior from Equation (1) and uses conjugacy, from which we have:

\begin{align}
p\left(\mathbf{\lambda} \big| \mathbf{g}^{1:t}\right) & \propto \prod_{j=1}^{t} p\left(\mathbf{g}^{j} \big| \mathbf{\lambda}\right) p\left(\mathbf{\lambda}\right) \nonumber \\
& \propto \prod_{i=1}^{m}\lambda_{i}^{\alpha-1}e^{-\beta \lambda_i} \prod_{j=1}^{t}\prod_{i=1}^{m} \theta \lambda_i e^{-\theta\lambda_i g_{i}^{j}} \nonumber \\
& = \prod_{i=1}^{m} \lambda_{i}^{\alpha-1}e^{-\beta\lambda_i}\lambda_{i}^{t}e^{-\theta\lambda_{i} \sum_{j=1}^{t} g_{i}^{j}} \nonumber \\
& = \prod_{i=1}^{m} \lambda_{i}^{(t + \alpha) - 1} e^{-\left(\beta + \theta\sum_{j=1}^{t}g_{i}^{j}\right)\lambda_{i}}
\end{align}

...which is the same result as above.

 In any case, with the posterior distribution in hand, following the discussion in the prior section, the update for each ensemble weight $\lambda_i$ at each time $t$ is the posterior mean of thes distributions, given by:

\begin{align}
\frac{t + \alpha}{\beta + \theta\sum_{j=1}^{t}g_{i}^{j}}
\end{align}

This update is intuitive. Because $\alpha$, $\beta$, and $\theta$ are all positive and $t$ is growing linearly over time, it is easy to see that the weight of each classifier in the ensemble depends inversely on its historical performance in terms of the sum of its historical losses on each incoming sample. Classifiers that have historically performed poorly will have their predictions appropriately downweighted (since $\sum_{j=1}^{t}g_{i}^{j}$ will be relatively large compared to the other classifiers), while the converse will be true for strong performers. Note that all information up to the current time is incorporated into each estimate.

This completes the derivations of the algorithm as stated in the paper. We will now use this particular form of the algorithm to run experiments on benchmark datasets.

# Empirical

This section reproduces experiments performed in the Section 5 of the paper ("Experiments") on test datasets to compare the Bayesian method proposed to other ensembling techniques. Our results match those of the authors closely. Data utilized were from five benchmark classification datasets ("heart", "mushrooms", "australian", "ionosphere", and "sonar") available here: https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html. All datasets present a binary classification problem. Full (Python) code for the experiment can be found in the appendix. (Note that the datasets as downloaded from the website are in sparse matrix format, which are most easily read using the scikit-learn function sklearn.datasets.load_svmlight_file.)

Each experiment runs as follows: 

For each trial, the test dataset in question is randomly split into an initial training portion (we use 10% of the available data, as is used in the paper) and a testing portion (the remaining 90% of the data) to be processed a single sample at a time in an online fashion. A single baseline naive bayes classifier that uses all the features is created, and a single ensemble of one-hundred naive bayes classifiers is created. To ensure differentiation in the classifiers in the ensemble, each is created to use a random subset of half of the available features in the data. The authors do not state in their paper how many features are used for their ensemble classifiers, and the number of features between datasets differs widely. This may be a source of small differences in results, though it does not seem to make a significant difference. This ensemble is then shared among the three ensembling methods considered:

1. "Voting" uses soft voting (class chosen by $argmax$ of the sums of the probability estimates for each class from each classifier in the ensemble).
2. "SGD" estimates the ensemble weights by stochastic gradient descent.
3. "Bayes" estimates the ensemble weights by the method described in this paper and make the prediction by the $argmin$ of the sums of the weighted losses for each class. Hyperparameters are set at $\alpha = 1$, $\beta = 1$, and $\theta = .1$. These are the values used in the final analysis given in the paper, and the authors show that the results of the experiments are practically invariant to the values of the hyperparameters, so we omit any tuning of them.

In brief, the outline of the experiment is given below:

* Initialize: one baseline classifier, one ensemble
* For trials $i = 1, \ldots, T$:
    1. Split the dataset into initial training (10%) and incoming testing (90%). Suppose the testing data has $N$ samples
    2. Train all classifiers on the original data. Classifiers in the ensemble are trained on their particular subset of the features
    3. For samples $(\mathbf{X}_j, y_j)$, $j = 1, \ldots, N$:
        * Make a prediction for $y_j$ using the features $\mathbf{X}_j$ for each method
        * Reveal $y_j$ and partially fit each classifier on the new sample. Record cumulative error rate up to time $j$
    4. Calculate the overall error rate on the incoming samples and store cumulative error rates as time progresses

Three main conclusions are evident:

* Ensembling methods do not always appear warranted, and often do not improve significantly upon the single online classifier that uses all the features. This is especially true in datasets where most of the features seem to be relevant (e.g., the "heart" dataset), as the performance of the ensembling classifiers that use only a subset of them will suffer significantly, often more than the reduced variance from ensembling can overcome. The uniform voting scheme performance deteriorates rapidly in this setting, since it has no way of weighting the classifiers in the ensemble that are performing well relative to the others. This is to be expected; the ensembling methods in use are completely contrived and require no new information or assumptions about the data to function, so we should not expect them to universally improve upon a single classifier. It does seem to be true, though, that the error rates on each trial are much more consistent for the ensemble methods (that is, they are less variable) than they are for the single classifier (the single classifier can often perform very well or very poorly relative to the usual performance for the given dataset, depending upon the trial). This, again, is to be expected, and is indeed one of the main benefits of ensembling.
* The Bayesian method almost uniformly outperforms both the uniform voting ensemble and the stochastic gradient descent method for estimating the ensemble weights on each trial and each dataset. It also seems to be a somewhat stable trend among the cumulative error rates (at least on the datasets where ensembling seems to make a difference) that the Bayesian method reaches its minimum error rate earlier in the online training stream than do the other methods. This is consistent with results presented in the paper that show the Bayesian method, under certain regularity conditions, is guaranteed to converge to a global minimum in the ensemble loss function, an attractive quality that neither voting nor SGD possesses.
* The Bayesian method seems to perform particularly strongly in high-dimensional datasets, functioning as a sort of feature-selection process for weighting the classifiers in the ensemble that contain the important features. This is evident in the "mushrooms" dataset, which contains 112 features, and on which the Bayesian ensemble consistently improves upon the single baseline classifier by about a percentage point, but the uniform voting ensemble consistently underperforms it by about the same margin. This appears to be because the subsetting of features harms the ensemble significantly when it cannot reweight itself towards the classifiers that are based upon the more important features.


```{r}
#'

suppressPackageStartupMessages(library(data.table))
suppressPackageStartupMessages(library(ggplot2))
suppressPackageStartupMessages(library(knitr))
suppressPackageStartupMessages(library(scales))

root <- '/Users/zgallegos/documents/school/math_538/project/data/results/'
acc_path <- paste0(root, 'accuracy/')
cum_path <- paste0(root, 'cumulative/')
loss_path <- paste0(root, 'loss/')

read <- function(x, id)
{
    d <- read.csv(x, header = TRUE)
    d$X <- NULL
    melt(d, id = id, value.name = 'error', variable.name = 'Method')
}

err_plot <- function(dataset)
{

    fl <- paste0(acc_path, dataset, '_errors.csv')

    data <- read(fl, 'trial')

    proper <- gsub('(?<=\\b)([a-z])', '\\U\\1', tolower(dataset), perl = TRUE)
    ttl <- paste0(proper, ' - Error Rates By Method/Trial')

    y_lower <- .975 * min(data$error)
    y_max <- 1.025 * max(data$error)

    data$trial <- data$trial + 1

    g <- ggplot(data = data,
            aes(x = as.factor(trial), y = error, fill = Method, group = Method))
    g <- g + geom_bar(stat = 'identity', position = 'dodge')
    # g <- g + geom_text(aes(label = percent(error)), 
    #                     position = position_dodge(width = 1),
    #                     family = 'serif',
    #                     vjust = 0,
    #                     size = 3)
    g <- g + coord_cartesian(ylim = c(y_lower, y_max))
    g <- g + theme_bw() + theme(text = element_text(family = 'serif'))
    g <- g + ggtitle(ttl) + xlab('Trial') + ylab('Error')
    g

}

cum_plot <- function(dataset, trial)
{

    fl <- paste0(cum_path, dataset, '_cumerrors_trial_', trial, '.csv')
    data <- read(fl, 'index')

    proper <- gsub('(?<=\\b)([a-z])', '\\U\\1', tolower(dataset), perl = TRUE)
    ttl <- paste0(proper, ' - Cumulative Error, Trial ', trial + 1)

    g <- ggplot(data = data, aes(x = index, y = error, colour = Method))
    g <- g + geom_line()
    g <- g + theme_bw() + theme(text = element_text(family = 'serif'))
    g <- g + ggtitle(ttl) + xlab('Sample') + ylab('Cumulative Error')
    g

}

loss_plot <- function(dataset, trial)
{

    proper <- gsub('(?<=\\b)([a-z])', '\\U\\1', tolower(dataset), perl = TRUE)
    ttl <- paste0(proper, ' Cumulative Errors and Ensemble Weights',
                  'for Selected Ensemble Classifiers, ', trial + 1)

    loss_fl <- paste0(loss_path, dataset, '_loss_samp_', trial, '.csv')
    lam_fl <- paste0(loss_path, dataset, '_lam_samp_', trial, '.csv')

    ind <- 1:4

    loss <- read.csv(loss_fl)
    for(i in ind[-1]) loss[,i] <- cumsum(loss[,i])
    loss$what <- 'Cumulative Loss'

    lam <- read.csv(lam_fl)
    lam$what <- 'Ensemble Weight'

    names(loss)[ind] <- names(lam)[ind] <- c('Sample', 'Worst', 'Median', 'Best')

    loss <- melt(loss, id = c('Sample', 'what'), variable.name = 'Classifier')
    lam <- melt(lam, id = c('Sample', 'what'), variable.name = 'Classifier')

    plt <- rbind(lam, loss)

    plt$what <- factor(plt$what, 
                        levels = c('Ensemble Weight', 'Cumulative Loss'),
                        ordered = TRUE)
    
    plt$Classifier <- factor(plt$Classifier,
                        levels = c('Best', 'Median', 'Worst'),
                        ordered = TRUE)

    g <- ggplot(data = plt, aes(x = Sample, y = value, colour = Classifier))
    g <- g + geom_line() + facet_grid(what ~ ., scales = 'free')
    g <- g + ggtitle(ttl)
    g <- g + theme_bw() + theme(text = element_text(family = 'serif'),
                                axis.title.y = element_blank())
    g

}


mns <- function(dataset)
{

    fl <- paste0(acc_path, dataset, '_errors.csv')
    proper <- gsub('(?<=\\b)([a-z])', '\\U\\1', tolower(dataset), perl = TRUE)

    data <- as.data.table(read(fl, 'trial'))
    data$Dataset <- proper

    data[, .(`Mean Error` = mean(error)), by = .(Dataset, Method)]

}


```

# Average Error Rate by Method

The table below shows the average error rate for incoming one-step classifications made by each classifier at the end of the experiment. Ten trials were performed for each dataset.

```{r}

datasets <- c('australian', 'ionosphere', 'heart', 'sonar')

tbl <- mns(datasets[1])

for(i in datasets[-1]) tbl <- rbind(tbl, mns(i))

tbl <- dcast(tbl, Dataset ~ Method, value.var = 'Mean Error')

names(tbl) <- c('Dataset', 'Single Classifier', 'Voting Ensemble',
                'Bayesian Weighting', 'SGD Weighting')

kable(tbl)

```

We notice that the Bayesian weighting scheme produces the lowest average error rate



```{r}

cum_plot('australian', 6)

```

```{r}

loss_plot('australian', 6)

```

## Dataset - Mushrooms

```{r}

err_plot('mushrooms')

```

```{r}

mns('mushrooms')

```

```{r}

cum_plot('mushrooms', 4)

```

## Dataset - Ionosphere

```{r}

err_plot('ionosphere')

```

```{r}

mns('ionosphere')

```

```{r}

cum_plot('ionosphere', 4)

```

```{r}

loss_plot('ionosphere', 4)

```

## Dataset - Heart

```{r}

err_plot('heart')

```

```{r}

mns('heart')

```

```{r}

cum_plot('heart', 2)

```

```{r}

loss_plot('heart', 2)

```

## Dataset - Sonar

```{r}

err_plot('sonar')

```

```{r}

mns('sonar')

```

```{r}

cum_plot('sonar', 0)

```

```{r}

loss_plot('sonar', 0)

```

# Appendix

## Empirical Section Full Code

The results shown in the Empirical section were produced by the following Python code:

```{python, eval = FALSE}

"""
not run
"""

import numpy as np
import pandas as pd
import os
import re

from sklearn import naive_bayes
from sklearn.base import clone
from sklearn.model_selection import train_test_split
from sklearn.metrics import log_loss, accuracy_score
from sklearn.datasets import load_svmlight_file
from sklearn.linear_model import SGDClassifier


def create_ensemble(classifier, X, num, num_features):
    """
    create a uniform ensemble of <num> classifiers, each of which uses
    <num_features> randomly-sampled features of the data
    """
    estimators = [None] * num
    n_feats = X.shape[1]

    for i in range(num):

        estimators[i] = clone(classifier)
        estimators[i].sub_feats = list(np.sort(np.random.choice(
            range(n_feats), size = num_features, replace = False)))

    return estimators


def fit_ensemble(ensemble, x, y, partial = False, classes = None):
    """
    fit the ensemble, each classifier with its own set of features
    if <partial>, do an online fit
    """
    for e in ensemble:

        x_ = x[:, e.sub_feats]
        
        if not partial:

            e.fit(x_, y)

        else:

            e.partial_fit(x_, y, classes = classes)

    return ensemble


def fit_sgd_ensemble(ensemble, X, y):
    sgd = SGDClassifier(loss='log')
    ensemble_prediction = []
    for e in ensemble:

        X_ = X[:, e.sub_feats]

        e.fit(X_, y)
        ensemble_prediction.append(e.predict_proba(X_)[:,1])
    return sgd.fit(np.array(ensemble_prediction).transpose(),y)


def predict_sgd(ensemble, fitted_sgd, X):
    """
    """
    ensemble_preds = [e.predict_proba(X[:, e.sub_feats])[:,1][0] for e in ensemble]
    return fitted_sgd.predict(np.array(ensemble_preds).reshape(1,-1))


def update_bayes(ensemble, i, x, y, classes, a, b, theta):
    """
    update loss and ensemble weights
    """
    t = i + 1

    for e in ensemble:

        x_ = x[:, e.sub_feats]
        pred = e.predict_proba(x_)
        e.loss[i] = log_loss(y, pred, labels = classes)

        num = a + t
        den = b + theta * np.sum(e.loss[0:t])
        e.lam[i] = num / den

    return ensemble


def predict_bayes(ensemble, i, X, classes):
    """
    ensemble prediction using posterior ensemble weights
    """
    m = len(ensemble)
    neg_label, pos_label = classes
    
    if i == -1:

        return predict_vote(ensemble, X, classes)

    curr_lams = np.array([e.lam[i] for e in ensemble])

    loss_neg = [None] * m
    loss_pos = [None] * m

    for i, e in enumerate(ensemble):

        pred = e.predict_proba(X[:, e.sub_feats])

        loss_neg[i] = log_loss([neg_label], pred, labels = classes)
        loss_pos[i] = log_loss([pos_label], pred, labels = classes)

    loss_neg = np.array(loss_neg)
    loss_pos = np.array(loss_pos)

    probs = [sum(curr_lams * loss_neg), sum(curr_lams * loss_pos)]

    return classes[np.argmin(probs)]


def predict_vote(ensemble, X, classes):
    """
    ensemble prediction by soft voting
    """
    ensemble_preds = [e.predict_proba(X[:, e.sub_feats]) for e in ensemble]
    pred = np.concatenate(ensemble_preds).sum(axis = 0)

    return classes[np.argmax(pred)]


def online_test(ensemble, single, X, y, classes, start_prop, **kwargs):
    """
    perform the tests
    """
    X_start, X_stream, y_start, y_stream = train_test_split(
        X, y, test_size = 1 - start_prop)

    stream_size = X_stream.shape[0]

    single_preds = [None] * stream_size
    vote_preds = [None] * stream_size
    bayes_preds = [None] * stream_size
    sgd_preds = [None] * stream_size

    for e in ensemble:

        e.loss = [None] * stream_size
        e.lam = [None] * stream_size


    """
    initial fit
    """

    single.fit(X_start, y_start)
    ensemble = fit_ensemble(ensemble, X_start, y_start)
    fitted_sgd = (fit_sgd_ensemble(ensemble, X_start, y_start))

    _X_ = X_start
    _y_ = y_start

    for j in range(stream_size):

        x_ = X_stream[j].reshape(1, -1)
        y_ = y_stream[j].reshape(1,)

        """
        update bayes loss and weights
        """

        ensemble = update_bayes(ensemble, j, x_, y_, classes, **kwargs)

        """
        make predictions
        """

        single_preds[j] = single.predict(x_)
        vote_preds[j] = predict_vote(ensemble, x_, classes)
        bayes_preds[j] = predict_bayes(ensemble, j - 1, x_, classes)
        sgd_preds[j] = predict_sgd(ensemble, fitted_sgd, x_)[0]

        """
        partial fit
        """

        single.partial_fit(x_, y_, classes = classes)
        ensemble = fit_ensemble(ensemble, x_, y_, partial = True, classes = classes)

        _X_=(np.vstack((_X_,x_)))
        _y_=(np.append(_y_,y_))

        fitted_sgd = fit_sgd_ensemble(ensemble, _X_, _y_)

    single_preds = np.array(single_preds).reshape(y_stream.shape)
    vote_preds = np.array(vote_preds).reshape(y_stream.shape)
    bayes_preds = np.array(bayes_preds).reshape(y_stream.shape)
    sgd_preds = np.array(sgd_preds).reshape(y_stream.shape)

    return single_preds, vote_preds, bayes_preds, sgd_preds, y_stream



if __name__ == '__main__':

    """
    run the tests. five data files, ten trials each
    """

    os.chdir('/users/zgallegos/documents/school/math_538/project/data')

    use_datasets = ['heart.txt', 'australian.txt', 'ionosphere.txt', 
                    'sonar.txt', 'mushrooms.txt',]
    use_classes = [(-1, 1), (-1, 1), (-1, 1), (-1, 1), (1, 2)]

    n_trials = 10
    feat_perc = .5 # percentage of the features ensemble classifiers get

    for j, k in enumerate(use_datasets):
        
        fl_name = re.search('^.+?(?=\.txt)', k).group(0)
        err_fl = fl_name + '_errors.csv'

        cls = use_classes[j]

        X, y = load_svmlight_file(k)
        X = X.toarray()
        y = y.astype(int)

        feats = int(np.floor(feat_perc * X.shape[1]))

        sing = []
        vote = []
        bayes = []
        sgd = []

        trlz = range(n_trials)

        for i in trlz:

            cumerr_fl = fl_name + '_cumerrors_trial_%s.csv' % i

            single = naive_bayes.BernoulliNB()
            ensemble = create_ensemble(naive_bayes.BernoulliNB(), X, 100, feats)

            s, v, b, d, y_ = online_test(ensemble, single, X, y, cls, 
                            start_prop = .1, a = 1, b = 1, theta = .1)

            indx = range(1, len(y_) + 1)
            cum_sing = 1 - np.cumsum(s == y_) / indx
            cum_vote = 1 - np.cumsum(v == y_) / indx
            cum_bayes = 1- np.cumsum(b == y_) / indx
            cum_sgd = 1- np.cumsum(d == y_) / indx

            to_write = np.stack((indx, cum_sing, cum_vote, cum_bayes, cum_sgd), axis = 1)
            
            df = pd.DataFrame(to_write, columns = ['index', 'single', 'vote', 'bayes', 'sgd'])
            df.to_csv(os.path.join('results', 'cumulative', cumerr_fl))

            sing.append(1 - accuracy_score(y_, s))
            vote.append(1 - accuracy_score(y_, v))
            bayes.append(1 - accuracy_score(y_, b))
            sgd.append(1 - accuracy_score(y_, d))

        to_write = np.stack((trlz, sing, vote, bayes, sgd), axis = 1)

        df = pd.DataFrame(to_write, columns = ['trial', 'single', 'vote', 'bayes', 'sgd'])
        df.to_csv(os.path.join('results', 'accuracy', err_fl))

```